# CV-深度学习基础知识 (4)

## **📌 23. 什么是 RNN（循环神经网络）？**

**循环神经网络（RNN, Recurrent Neural Network）** 是专门用于处理**序列数据**的神经网络，它能够**记住历史信息**，并用于当前计算。

**普通神经网络（如 CNN）** 每次处理一个独立的输入，没有“记忆”功能。

**RNN** **可以记住之前的信息**，并在当前计算时使用它。

### **✅ RNN 的核心特点**

1. **处理时序数据**：
   - **输入数据是序列**（如文本、语音、时间序列）。
   - **输出也是序列**（如翻译、语音生成）。
2. **记忆过去的信息（隐藏状态）**：
   - RNN **不仅依赖当前输入**，还会使用**上一步的计算结果**（隐藏状态）。
3. **适用于时间序列任务**：
   - **视频分类**：对每一帧进行分类（输入 = 每帧图像，输出 = 类别）。
   - **文本生成（Char RNN）**：输入字符，预测下一个字符。

🔥 **PyTorch 代码**

```python
import torch.nn as nn

rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
```

------



## **📌 24. LSTM 结构，如何缓解梯度消失？**

### **1. LSTM（长短时记忆网络）是什么？**

LSTM（Long Short-Term Memory）是**RNN 的改进版本**，可以**长期记住重要信息，忘记不重要信息**，解决**梯度消失问题**。



### **2. 为什么 RNN 会梯度消失？**

- RNN 只能“短期记忆”，**时间久了会忘记之前的信息**。
- 例如：你听到**一句很长的话**，如果要复述开头的内容，可能已经忘记了。
- **梯度消失 = 训练时间再长，也无法学到长序列关系**。



### **3. LSTM 结构**

LSTM 通过 **三个门（Gate）** 解决梯度消失问题：

- LSTM 像人记笔记✍：
  - 重要的内容 ✅ → 记下来（保留长时间记忆）。
  - 不重要的内容 ❌ → 忘掉（减少干扰）。

**✅ 关键结构**

1. **遗忘门（Forget Gate）**
   - 负责决定哪些信息需要丢弃，比如：
   - “背景描述”可能不重要 → 忘掉。
   - “核心事件”很重要 → 记住。
2. **输入门（Input Gate）**
   - 决定哪些新信息要加入记忆，比如：
   - “最新发生的关键事件” → 记住。
3. **输出门（Output Gate）**
   - 选择哪些记忆要输出，比如：
   - “只输出最重要的信息”。

### **4. 为什么 LSTM 能缓解梯度消失？**

- 在我们 LSTM 中的第⼀步是决定我们会从细胞状态中丢弃什么信息。这个决定通过⼀ 个称为“忘记门”的结构完成。

- LSTM 的内部结构，通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；

- 相当于记住了重要的，忘记了⽆关紧要的

- **普通 RNN：**

  ht=f(Whht−1+Wxxt)h_t = f(W_h h_{t-1} + W_x x_t)

  - **如果 W_h 太小，梯度会越来越小（梯度消失）。**
  - **如果 W_h 太大，梯度会爆炸。**

- **LSTM 通过 C_t 直接传递信息，避免梯度消失**：

  Ct=ft⋅Ct−1+it⋅C~tC_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t

  - **有了遗忘门，梯度不会消失**。
  - **重要信息可以长期存储**，不会丢失。

🔥 **PyTorch 代码**

```python
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
```

------



## **📌 25. GRU vs. LSTM**

### **直观理解**

- **LSTM**：记忆力好，但**复杂度高**，有**3 个门**（输入、遗忘、输出门）。
- <u>**GRU**：更简单，计算更快</u>，只有 **2 个门**（更新门、重置门）。

**GRU（Gated Recurrent Unit）是 LSTM 的改进版本**，它的计算更简单，但效果和 LSTM 相似。

| **对比项**     | **GRU**                      | **LSTM**                               |
| -------------- | ---------------------------- | -------------------------------------- |
| **门控结构**   | **2 个门**（更新门、重置门） | **3 个门**（输入门、遗忘门、输出门）   |
| **隐藏状态**   | 只有一个隐藏状态 hth_t       | 既有隐藏状态 hth_t，又有细胞状态 CtC_t |
| **计算复杂度** | **更快**，参数更少           | **更复杂**，参数更多                   |
| **适用场景**   | 适用于大规模 NLP 任务        | 适用于需要长序列记忆的任务             |

### **哪种更好？**

| **情况**             | **选择**         |
| -------------------- | ---------------- |
| **数据较少**         | GRU（计算快）    |
| **长时间依赖的任务** | LSTM（记忆力强） |
| **需要高效训练**     | GRU（参数少）    |

🔥 **PyTorch 代码**

```python
gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
```

------



## **📌 26. 什么是 Softmax？**

Softmax 是**用于多分类任务的激活函数**，它能把多个神经元的输出**转换为概率分布**。

### **直观理解**

> Softmax 就像**投票系统** 🗳：
>
> - 它把所有候选类别的“分数”转换成“概率”。
> - **得票最多的类别**（概率最大）就是最终的预测结果。

### **✅ Softmax 的作用**

1. <u>**将多个输出转换为概率分布**。</u>
2. <u>**适用于多分类任务（如 ImageNet 分类）**。</u>
3. **概率最高的类别即为预测类别**。

🔥 **PyTorch 代码**

```python
import torch.nn.functional as F

logits = torch.tensor([2.0, 1.0, 0.1])  # 假设神经网络的原始输出
probabilities = F.softmax(logits, dim=0)
print(probabilities)  # 归一化后的概率
```

### **总结**

| **问题**                      | **答案总结**                              |
| ----------------------------- | ----------------------------------------- |
| **23. RNN 是什么？**          | 记住过去的信息，适用于语音、翻译等任务    |
| **24. LSTM 解决了什么问题？** | 解决 RNN 记忆力差的问题，通过“门”控制记忆 |
| **25. GRU vs. LSTM**          | GRU 更简单，计算快，LSTM 记忆力强         |
| **26. 什么是 Softmax？**      | 把输出转换成概率分布，用于多分类          |

- RNN 适用于序列任务，但存在梯度消失问题。
- LSTM 通过门控机制解决了梯度消失，使模型能够学习长时依赖。
- GRU 是 LSTM 的简化版本，计算更快。
- Softmax 用于多分类问题，它把神经网络的输出转换成概率分布，最终选取概率最高的类别作为预测结果。



## **📌 27. 为什么 ReLU 比 Sigmoid 和 Tanh 更好？**

在神经网络中，激活函数的作用是**引入非线性**，让网络能够学习复杂的模式。常见的激活函数有 **Sigmoid, Tanh, ReLU**，<u>但 **ReLU** 是目前最常用的，因为它有**计算效率高、梯度更稳定、不易梯度消失**等优点。</u>

### **1. 计算效率：ReLU 计算比 Sigmoid 和 Tanh 更快**

- **Sigmoid 和 Tanh 计算复杂**，涉及**指数运算（exp）**：

  Sigmoid(x)=1/(1+e−x)

  Tanh(x)=(ex−e−x)/(ex+e−x) 

  - **缺点**：计算成本高，求梯度时涉及除法，计算速度较慢。

- **ReLU 计算简单**：

  ReLU(x)=max⁡(0,x) 

  - **优点**：只需比较 `0` 和 `x`，计算量小，**加速训练**。

------



### **2. ReLU 避免梯度消失**

**Sigmoid 和 Tanh 的梯度范围很小**：

- **Sigmoid 梯度范围**：最大值 **0.25**，接近 0 时梯度极小 → **容易梯度消失**。
- **Tanh 梯度范围**：最大值 **1**，但远离 0 时梯度仍然接近 0。

**ReLU 梯度在正区间恒为 1**：

- **不会梯度消失**，能稳定传播梯度，提高深度网络的训练效果。

------

### **3. ReLU 让网络更稀疏，减少过拟合**

- ReLU 具有“神经元死亡”特性：
  - 当输入 x<0x < 0 时，ReLU 输出 0，相当于让一部分神经元“休眠”。
  - **减少参数相互依赖，提高泛化能力，降低过拟合风险**。

🔥 **PyTorch 代码**

```python
import torch.nn.functional as F

output = F.relu(input_tensor)  # ReLU 激活
```

------

### **🚀 结论**

| **激活函数** | **计算复杂度**    | **梯度消失问题** | **适用场景**               |
| ------------ | ----------------- | ---------------- | -------------------------- |
| **Sigmoid**  | 高（指数运算）    | 容易梯度消失     | 仅适用于二分类             |
| **Tanh**     | 中等（指数运算）  | 仍然会梯度消失   | 适用于小型网络             |
| **ReLU**     | 低（仅 max 操作） | 不会梯度消失     | 适用于深度网络（默认选项） |

------



