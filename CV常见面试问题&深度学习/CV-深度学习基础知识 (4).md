# CV-深度学习基础知识 (4)

## **📌 23. 什么是 RNN（循环神经网络）？**

**循环神经网络（RNN, Recurrent Neural Network）** 是专门用于处理**序列数据**的神经网络，它能够**记住历史信息**，并用于当前计算。

**普通神经网络（如 CNN）** 每次处理一个独立的输入，没有“记忆”功能。

**RNN** **可以记住之前的信息**，并在当前计算时使用它。

### **✅ RNN 的核心特点**

1. **处理时序数据**：
   - **输入数据是序列**（如文本、语音、时间序列）。
   - **输出也是序列**（如翻译、语音生成）。
2. **记忆过去的信息（隐藏状态）**：
   - RNN **不仅依赖当前输入**，还会使用**上一步的计算结果**（隐藏状态）。
3. **适用于时间序列任务**：
   - **视频分类**：对每一帧进行分类（输入 = 每帧图像，输出 = 类别）。
   - **文本生成（Char RNN）**：输入字符，预测下一个字符。

🔥 **PyTorch 代码**

```python
import torch.nn as nn

rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
```

------



## **📌 24. LSTM 结构，如何缓解梯度消失？**

### **1. LSTM（长短时记忆网络）是什么？**

LSTM（Long Short-Term Memory）是**RNN 的改进版本**，可以**长期记住重要信息，忘记不重要信息**，解决**梯度消失问题**。



### **2. 为什么 RNN 会梯度消失？**

- RNN 只能“短期记忆”，**时间久了会忘记之前的信息**。
- 例如：你听到**一句很长的话**，如果要复述开头的内容，可能已经忘记了。
- **梯度消失 = 训练时间再长，也无法学到长序列关系**。



### **3. LSTM 结构**

LSTM 通过 **三个门（Gate）** 解决梯度消失问题：

- LSTM 像人记笔记✍：
  - 重要的内容 ✅ → 记下来（保留长时间记忆）。
  - 不重要的内容 ❌ → 忘掉（减少干扰）。

**✅ 关键结构**

1. **遗忘门（Forget Gate）**
   - 负责决定哪些信息需要丢弃，比如：
   - “背景描述”可能不重要 → 忘掉。
   - “核心事件”很重要 → 记住。
2. **输入门（Input Gate）**
   - 决定哪些新信息要加入记忆，比如：
   - “最新发生的关键事件” → 记住。
3. **输出门（Output Gate）**
   - 选择哪些记忆要输出，比如：
   - “只输出最重要的信息”。

### **4. 为什么 LSTM 能缓解梯度消失？**

- 在我们 LSTM 中的第⼀步是决定我们会从细胞状态中丢弃什么信息。这个决定通过⼀ 个称为“忘记门”的结构完成。

- LSTM 的内部结构，通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；

- 相当于记住了重要的，忘记了⽆关紧要的

- **普通 RNN：**

  ht=f(Whht−1+Wxxt)h_t = f(W_h h_{t-1} + W_x x_t)

  - **如果 W_h 太小，梯度会越来越小（梯度消失）。**
  - **如果 W_h 太大，梯度会爆炸。**

- **LSTM 通过 C_t 直接传递信息，避免梯度消失**：

  Ct=ft⋅Ct−1+it⋅C~tC_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t

  - **有了遗忘门，梯度不会消失**。
  - **重要信息可以长期存储**，不会丢失。

🔥 **PyTorch 代码**

```python
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
```

------


