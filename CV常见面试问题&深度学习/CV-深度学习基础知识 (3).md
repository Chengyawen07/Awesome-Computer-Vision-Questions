# CV-深度学习基础知识 (3)

## **📌 13. 为什么先用 Adam，然后再用 SGD？**

✅ **原理**

- **SGD（随机梯度下降）**：每次更新参数时，只用一个小批量（Mini-Batch）的样本计算梯度，并沿着梯度方向下降。
  - lr固定
  - 计算简单，效率高

- **Adam（Adaptive Moment Estimation）**：是**SGD 的升级版**，它结合了**Momentum（动量）和 RMSprop（自适应学习率）**，能**自适应调整学习率**，加速收敛
  - **自适应调整学习率，不同参数有不同的学习率**
  - 能加速收敛





✅ **Adam 收敛快，但最终效果不如 SGD**

- **Adam** 是自适应优化算法，前期收敛速度**比 SGD 快**，适合快速找到一个较优的解。
- 但在训练后期，**Adam 的学习率会变得很小**，可能会卡在次优解附近，无法继续优化。



✅ **SGD 适合精调，让模型收敛到最优解**

- **SGD** 在训练后期可以更稳定地找到最优解，泛化能力更强。
- 所以可以**先用 Adam 快速收敛**，然后**切换到 SGD 进行精调**，获得更好的最终效果。

🔥 **PyTorch 代码示例**

```python
import torch.optim as optim

# 先使用 Adam 训练
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 后期换 SGD
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```



### ✅ **什么时候用 Adam，什么时候用 SGD？**

| **情况**                    | **推荐算法** | **原因**                                         |
| --------------------------- | ------------ | ------------------------------------------------ |
| **前期快速收敛**            | **Adam**     | Adam 适合大数据集，能自适应调整学习率，加速训练  |
| **后期精调**                | **SGD**      | SGD 在训练后期，能更稳定找到最优解，泛化能力更强 |
| **小数据集**                | **SGD**      | Adam 的自适应特性在小数据集上可能没有优势        |
| **NLP 任务（Transformer）** | **Adam**     | NLP 需要快速训练，Adam 效果更好                  |



## **📌 14. 什么是周期学习率（Cyclical Learning Rate, CLR）？**

✅ **学习率是超参数，影响训练效果**

- 学习率太大 → 训练不稳定，难以收敛
- 学习率太小 → 训练速度慢，容易卡住

✅ **周期学习率（CLR）的作用**

- <u>让学习率**在一定范围内周期性变化**，而不是固定的。</u>
- 这样可以**跳出局部最优点，加速收敛**，提高模型泛化能力。

🔥 **CLR 公式**：

- Learning Rate= Min LR + (Max LR−Min LR) × f(cycle position)

其中 `f(cycle position)` 可以是 **三角波、余弦波等**。

🔥 **PyTorch 代码实现**

```python
from torch.optim.lr_scheduler import CyclicLR

optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, step_size_up=2000, mode='triangular')
```

------



## **📌 15. 如何判断神经网络过拟合？怎么缓解？**

✅ **如何判断过拟合？**

1. **训练误差下降，但测试误差变大** ✅
2. **学习曲线**：训练 Loss **持续下降**，但验证 Loss **先下降后上升**
3. <u>**训练数据准确率高（95%+），但测试数据准确率低**</u>



✅ **缓解过拟合的方法**

| **方法**                          | **作用**                                     |
| --------------------------------- | -------------------------------------------- |
| **增加训练数据**                  | 让模型学到更普遍的特征                       |
| **数据增强（Data Augmentation）** | 在图像任务中扩充数据，如旋转、镜像           |
| **使用正则化（L1/L2）**           | 限制参数大小，防止模型过拟合                 |
| **减少特征数**                    | 移除不相关特征，提高泛化能力                 |
| **调整超参数**                    | 降低学习率，增加 Batch Size                  |
| **降低模型复杂度**                | 减少神经元数，减少层数                       |
| **使用 Dropout**                  | 随机丢弃部分神经元                           |
| **提前停止（Early Stopping）**    | 训练时如果验证 Loss 不再下降，则提前停止训练 |

🔥 **PyTorch 代码（L2 正则化 + Dropout）**

```python
import torch.nn as nn
import torch.optim as optim

# 使用 Dropout
dropout_layer = nn.Dropout(p=0.5)

# 在 PyTorch 里，我们用 `weight_decay` 来控制 L2 正则化
optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)
```



✅ **L2 正则化（Weight Decay）**

- 在深度学习中，**L2 正则化**（也称为**权重衰减 Weight Decay**）的作用是**限制模型参数的大小**，防止过拟合。

- 也就是在Loss的函数后，加上一个正则项。比如Loss = MSE + L2正则

- 原理：

  - **模型过拟合 = 过度依赖某些特征，导致权重变得特别大**。
  - **L2 正则化**会**让权重变小**，防止网络学习到过于复杂的模式，增强泛化能力。

- 在 PyTorch 里，我们用 `weight_decay` 来控制 L2 正则化。

- 🔥 **适用于 SGD, Adam, RMSprop 等优化器**

  - optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)

  

- 

## **📌 16. 全卷积神经网络（FCN）的优点？**

✅ **全卷积神经网络（FCN, Fully Convolutional Network）**

- FCN 主要用于**图像分割任务（Semantic Segmentation）**，与传统 CNN 相比，它能在**像素级别**做预测，不仅仅是分类。

✅ **全卷积网络（FCN）特点**：

1. **端到端预测**：输入图像 → 输出像素级预测，不需要额外的分类步骤。
   1. 直接输入原始图像，网络自动学习特征，并输出像素级分类

2. **能处理任意大小的输入图像**
   1. **CNN 需要固定大小的输入图像**（如 224×224），而 FCN 可以接受**任意大小的图像**，并直接输出相同尺寸的预测结果。
   2. 传统 CNN 需要**全连接层（FC）**，但 FCN 移除了 FC 层，换成了**卷积层**，这样就可以处理任意大小的输入。

3. **适用于图像分割**：如医学图像、自动驾驶感知任务。
   1. **CNN 只能输出一个类别标签**（如“猫”或“狗”），但 FCN 能**预测每个像素的类别**（如每个像素属于“猫”或“背景”）。
   2. 适用于**语义分割（Semantic Segmentation）\**任务，比如\**自动驾驶、医学影像分析**

4. 使用跳跃连接（Skip Connection），保留细节信息
   1. 由于卷积 + 池化会丢失细节，FCN **使用跳跃连接（Skip Connection）**，将**低级特征与高级特征融合**，提高分割精度。




✅ **FCN 适用于哪些任务？**

| **任务**         | **应用场景**         |
| ---------------- | -------------------- |
| **图像语义分割** | 目标检测、自动驾驶   |
| **医学影像分析** | 肿瘤检测、X-ray 分析 |
| **场景理解**     | 智能安防、无人机视觉 |

🔥 **PyTorch 代码（使用 FCN 进行语义分割）**

```python
import torchvision.models.segmentation as segmentation

# 预训练的 FCN 模型
model = segmentation.fcn_resnet50(pretrained=True)
```

------





## **📌 17. 1×1 卷积的作用**

✅ **1×1 卷积（pointwise convolution）**

- 1×1 卷积的作用本质上是**调整通道数（Channel）**，也就是**改变特征图（Feature Map）的深度维度**，但不改变特征图的空间尺寸（Height × Width）。
- 这里的 **"维度" 指的是通道数（Channels）**，而不是图像的宽度（Width）或高度（Height）。



**1. 主要用于 降维、升维。**

- **减少通道数**，降低计算量，加快推理速度。
- **示例**：
  - **输入通道数：512**
  - **使用 1×1 卷积降维到 128**
  - **输出通道数变小，但 H×W 不变**

```Python
import torch.nn as nn

conv1x1 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1)  # 降维
```



 **2. 增强特征组合能力（特征融合）**

- **1×1 卷积能重新组合特征通道信息**，类似于神经网络的**全连接层**，但计算量更低。
- 允许不同通道间的信息交互，**提升模型的表达能力**。



✅ **1×1 卷积的核心作用**

| **作用**                  | **解释**                             |
| ------------------------- | ------------------------------------ |
| **升/降维（通道数调整）** | 改变特征图的通道数，提高计算效率     |
| **增加网络深度**          | 增强非线性映射能力，提高学习能力     |
| **组合特征**              | 让不同通道的信息交互，学习更复杂特征 |

